# -*- coding: utf-8 -*-
"""twitter_tweets_classification_rnn_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T4ezOfwLhdJvX8xBMtFguUidnJndT9Sl
"""

# processed data csv path
# processed_tweets_path = '/content/drive/MyDrive/sentiment analysis files/tokenized_data_joined.csv'

# import pandas as pd

# load the processed data from the csv file
# data = pd.read_csv(processed_tweets_path).iloc[:, 1:]
# data.head()

"""#Actual code starts here with the pickle file

# pickling a dataframe to a pickle file
import pickle
path = ""
df = pd.DataFrame()

with open(path) as file_handle:
  pickle.dump(df, file_handle)
"""

!cp /content/drive/MyDrive/tokenized_data.pkl /content

!ls

import pandas as pd
from concurrent.futures import ThreadPoolExecutor
from keras.layers import TextVectorization
import numpy as np
import pickle

# with open("/content/drive/MyDrive/tokenized_data.pkl") as f:
reduced_data = pd.read_pickle('/content/tokenized_data.pkl')

reduced_data.head()

"""##**STEP - 1:** Find the length of the longest list in the text column"""

# find the maximum sequence length using ordinary for loop
max_len = 0
for text in reduced_data["text"]:
  temp = len(text)
  if temp > max_len:
    max_len = temp
print(max_len)

# find the maximum sequence length using apply function 
max_len = max(reduced_data["text"].apply(lambda x: len(x)))
print(max_len)

# find the maximum sequence length using map function, **turns out to be very slow**
# with ThreadPoolExecutor(max_workers=4) as pool:
#   max_len = max(list(pool.map(lambda x: len(x), list(reduced_data["text"]))))
# print(max_len)

def filter_by_len(token_list):
  if len(token_list) >= 10:
    return True
  return False

reduced_data["text"]

"""##**STEP-2:** Remove the tweets having less than 10 tokens, to reduce network training time"""

# reduced_data["text"] = list(filter(filter_by_len, list(reduced_data["text"])))
filtered_reduced_data = reduced_data[reduced_data["text"].apply(lambda x: len(x)) >= 10]

filtered_reduced_data.shape

"""###**WORD EMBEDDING:** Computer stores the meaning of a word in form of vectors called Word Embeddings."""

filtered_reduced_data

filtered_reduced_data["text"] = filtered_reduced_data["text"].apply(lambda x: " ".join(x))

filtered_reduced_data.head()

max_vocab_size = 30000
embedding_dim = 64    # the size of word embedding to be used

vectorization_layer = TextVectorization(max_tokens=max_vocab_size, output_sequence_length=max_len)

vectorization_layer.adapt(data=np.array(filtered_reduced_data["text"]))

pickle.dump({'config': vectorization_layer.get_config(),
             'weights': vectorization_layer.get_weights()}
            , open("/content/token_integer_mapping.pkl", "wb"))

# !mv /content/token_integer_mapping.pkl /content/drive/MyDrive

"""##**START FROM HERE**"""

!cp /content/drive/MyDrive/token_integer_mapping.pkl /content

!ls

with open("/content/token_integer_mapping.pkl", "rb") as file_handle:
  loaded_vectorization_layer = pickle.load(file_handle)

loaded_vectorization_layer

