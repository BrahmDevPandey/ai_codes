# -*- coding: utf-8 -*-
"""twitter_tweets_data_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15FDfPDgwmLwB7yT2SgG49zripQnbWzmG
"""

!cd /content    # switch  to /content directory

from google.colab import files    # for downloading dataset from kaggle

files.upload()

!mkdir ~/.kaggle

!mv ./kaggle.json ~/.kaggle

!chmod 600 ~/.kaggle/kaggle.json

!pip install kaggle

!kaggle datasets download -d kazanova/sentiment140

!unzip sen*

!pip install contractions

!pip install spacy

import pandas as pd   # to manage data of csv
from concurrent.futures import ThreadPoolExecutor   # for parallel processing
from multiprocessing import cpu_count   # to get the number of threads available in the threads
import contractions   # for performing contractions expansion on text
import re   # for regular expressions to remove useless symbols from the text
from nltk import word_tokenize
import nltk
from spacy.lang.en import stop_words
from nltk.corpus import stopwords
from nltk.corpus import wordnet as wn
from nltk.tag import pos_tag
from nltk.stem import WordNetLemmatizer

nltk.download("wordnet")

nltk.download("punkt")

nltk.download("stopwords")

stopwords = stopwords.words("english")
STOP_WORDS = stop_words.STOP_WORDS
en_stop_words = set.union(set(stopwords), set(STOP_WORDS))

cpu_count()

csv_path = "/content/training.1600000.processed.noemoticon.csv"
data = pd.read_csv(csv_path, encoding="latin-1", header=None)
data

data.shape
print(set(data[0]))

data.rename(columns={0: "target", 1: "ids", 2: "date", 3: "flag", 4: "user", 5: "text"}, inplace=True)
data

data.drop(columns=["ids", "date", "flag", "user"], inplace=True)
data

print(set(data[data.columns[0]]))

# function to normalize tweets text
def normalize_tweet(tweet_text):
  return tweet_text.lower()

# normalize the text of the tweets
with ThreadPoolExecutor(max_workers=4) as pool:
  data["text"] = list(pool.map(nor  malize_tweet, list(data["text"])))

data

regex_pattern = r'^@[a-zA-z0-9 ]+|^#[a-zA-Z0-9 ]+|\w+:\/{2}[\d\w-]+(\.[\d\w-]+)*(?:(?:\/[^\s/]*))*|\W+|\d+|<("[^"]*"|\'[^\']*\'|[^\'">])*>|_+|[^\u0000-\u007f]+'

def remove_noisy_tokens(tweet_text):
  return re.sub(pattern=regex_pattern, repl=" ", string=tweet_text)

with ThreadPoolExecutor(max_workers=4) as pool:
  data["text"] = list(pool.map(remove_noisy_tokens, list(data["text"])))

data["text"]

def remove_remaining_noisy_tokens(tweet_text):
  return re.sub(r'\b\w\b|[^\u0000-\u007f]+|_+|\W+', repl=" ", string=tweet_text)

with ThreadPoolExecutor(max_workers=4) as pool:
  data["text"] = list(pool.map(remove_remaining_noisy_tokens, list(data["text"])))

data["text"].head()

def tokenize_tweet_text(tweet_text):
  return word_tokenize(tweet_text)

with ThreadPoolExecutor(max_workers=4) as pool:
  data["text"] = list(pool.map(tokenize_tweet_text, list(data["text"])))

def is_stopword(token_text):
  return not(token_text in en_stop_words)

def remove_stopwords(tweet_tokens):
  return list(filter(is_stopword, tweet_tokens))

with ThreadPoolExecutor(max_workers=4) as pool:
  data["text"] = list(pool.map(remove_stopwords, list(data["text"])))

data["text"].head()

def translate_pos_tag(token_treebank_tag):
  if token_treebank_tag[1].startswith('J'):
    return (token_treebank_tag[0], wn.ADJ)
  elif token_treebank_tag[1].startswith('V'):
    return (token_treebank_tag[0], wn.VERB)
  elif token_treebank_tag[1].startswith('N'):
    return (token_treebank_tag[0], wn.NOUN)
  elif token_treebank_tag[1].startswith('R'):
    return (token_treebank_tag[0], wn.ADV)
  else:
    return (token_treebank_tag[0], wn.NOUN)

def tag_token(tweet_tokens):
  return list(map(translate_pos_tag, pos_tag(tweet_tokens)))

nltk.download('averaged_perceptron_tagger')

data["text"] = list(map(tag_token, list(data["text"])))

data["text"].head()

lemmatizer = WordNetLemmatizer()  # create an instance of wordnet lemmatizer

# function to lemmatize each token of the list
def lemmatize_token(token_pos_tuple):
  if token_pos_tuple == None:
    return ""
  else:
    return lemmatizer.lemmatize(word=token_pos_tuple[0], pos=token_pos_tuple[1])

def lemmatize_tweet_tokens(tweet_tokens):
  if len(tweet_tokens) > 0:
    return list(map(lemmatize_token, tweet_tokens))
  else:
    return [""]

data["text"] = list(map(lemmatize_tweet_tokens, list(data["text"])))

data["text"].head()

def join_tokens(tweet_tokens):
  return " ".join(tweet_tokens)

with ThreadPoolExecutor(max_workers=4) as pool:
  data["text"] = list(pool.map(join_tokens, list(data["text"])))

data["text"].head()

data

data.to_csv('/content/drive/MyDrive/sentiment analysis files/tokenized_data_joined.csv')

